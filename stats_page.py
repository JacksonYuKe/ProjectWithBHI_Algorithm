from dash import html, dcc, dash_table, Input, Output, State
import dash_bootstrap_components as dbc
import pandas as pd
import plotly.express as px
import glob
import os

import os
import glob
import pandas as pd
from sklearn.cluster import KMeans, MiniBatchKMeans
from sklearn.impute import SimpleImputer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

import os
import glob
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def process_weekly_csv(window_size, threshold):
    # input_dir = "/Users/jacson/BHI/Decrypted_Files/weekly"  # Updated directory
    input_dir = "/Users/jackson/BHI/Decrypted_Files/weekly_csv_files_test"  # Updated directory
    file_paths = sorted(glob.glob(os.path.join(input_dir, "week_2023-*.csv")))
    print(f"ğŸ§ Found {len(file_paths)} files")

    if len(file_paths) == 0:
        print("âŒ No CSV files found in the directory!")
        return pd.DataFrame(columns=["LOCATION", "Probability", "Weeks"]), 0, 0, 0, 0

    location_data = {}
    total_weeks = len(file_paths)

    # å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file in file_paths:
        try:
            week_number = os.path.basename(file).split("_")[1].split(".")[0]
            df = pd.read_csv(file, low_memory=False)
            df.columns = df.columns.str.strip()
            if "LOCATION" not in df.columns:
                print(f"âš ï¸ Skipping {file}: 'LOCATION' column not found!")
                continue

            # å¯¹æ¯ä¸ª location è¿›è¡Œå¤„ç†
            for location, group in df.groupby("LOCATION"):
                usage_matrix = group.iloc[:, 4:28].astype(float).values  # æå–è¯¥åœ°ç‚¹çš„24å°æ—¶ç”¨ç”µæ•°æ®

                # è®¡ç®—è¯¥ location çš„ baselineï¼šå³å¹³å‡æ¯å°æ—¶ç”¨ç”µé‡
                baseline = usage_matrix.mean()  # è®¡ç®—æ¯å‘¨24å°æ—¶çš„å¹³å‡å€¼
                baseline = baseline.mean()  # è®¡ç®—è¯¥åœ°ç‚¹çš„æ•´ä½“ baselineï¼ˆå¹³å‡æ¯å°æ—¶ï¼‰

                # åˆ¤æ–­æ˜¯å¦æœ‰è¿ç»­ window_size ä¸ªæ—¶é—´ç‚¹éƒ½è¶…è¿‡é˜ˆå€¼
                condition_met = False
                for row in usage_matrix:
                    # å¯¹æ¯ä¸€è¡Œ(æ¯å¤©)æ£€æŸ¥æ˜¯å¦æœ‰è¿ç»­window_sizeä¸ªæ—¶é—´ç‚¹éƒ½è¶…è¿‡é˜ˆå€¼
                    for i in range(len(row) - window_size + 1):
                        if all(point > (threshold + baseline) for point in row[i:i + window_size]):
                            condition_met = True
                            break
                    if condition_met:
                        break

                if location not in location_data:
                    location_data[location] = {"Exceed_Count": 0, "Weeks": [], "Baseline": baseline}

                if condition_met:
                    location_data[location]["Exceed_Count"] += 1
                    location_data[location]["Weeks"].append(week_number)

        except Exception as e:
            print(f"âŒ Error processing file {file}: {e}")

    # è®¡ç®—æ¦‚ç‡
    prob_df = pd.DataFrame([{
        "LOCATION": loc,
        "Probability": round(data["Exceed_Count"] / total_weeks, 3) if total_weeks > 0 else 0,
        "Weeks": ", ".join(data["Weeks"]),
        "Baseline": data["Baseline"]
    } for loc, data in location_data.items()])

    # è¯»å–çœŸå®å……ç”µæ¡©æ•°æ®
    real_data = pd.read_csv(file_paths[-1], usecols=["LOCATION", "# of Chargers"])
    real_data["Has_Charger"] = real_data["# of Chargers"].notna().astype(int)

    # åˆå¹¶é¢„æµ‹æ•°æ®å’ŒçœŸå®æ•°æ®
    merged_df = prob_df.merge(real_data, on="LOCATION", how="left").fillna(0)
    merged_df["Prediction"] = (merged_df["Probability"] > 0.5).astype(int)

    # åˆ›å»ºè¯„ä¼°æ•°æ®çš„å‰¯æœ¬ï¼Œé¿å…å½±å“åŸå§‹æ•°æ®
    eval_df = merged_df.copy()

    # è®¡ç®—æ ·æœ¬æ¯”ä¾‹å·®å¼‚
    tp_fn = eval_df[eval_df["Has_Charger"] == 1]  # æœ‰å……ç”µæ¡©çš„æ•°æ®
    fp_tn = eval_df[eval_df["Has_Charger"] == 0]  # æ²¡æœ‰å……ç”µæ¡©çš„æ•°æ®

    tp_fn_count = len(tp_fn)
    fp_tn_count = len(fp_tn)

    if tp_fn_count > fp_tn_count:
        tp_fn = tp_fn.sample(n=fp_tn_count, random_state=42)
    elif fp_tn_count > tp_fn_count:
        fp_tn = fp_tn.sample(n=tp_fn_count, replace=True, random_state=42)

    # åˆå¹¶å›ç¼©æ”¾åçš„æ•°æ®ï¼ˆä»…ç”¨äºè¯„ä¼°ï¼‰
    balanced_df = pd.concat([tp_fn, fp_tn])

    # é‡æ–°è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆä½¿ç”¨å¹³è¡¡åçš„æ•°æ®ï¼‰
    y_true = balanced_df["Has_Charger"]
    y_pred = balanced_df["Prediction"]

    accuracy = accuracy_score(y_true, y_pred) * 100
    precision = precision_score(y_true, y_pred) * 100
    recall = recall_score(y_true, y_pred) * 100
    f1 = f1_score(y_true, y_pred) * 100

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()

    print(f"True Positive (TP): {tp}")
    print(f"False Positive (FP): {fp}")
    print(f"False Negative (FN): {fn}")
    print(f"True Negative (TN): {tn}")

    print(f"âœ… Accuracy: {accuracy:.3f}")
    print(f"ğŸ¯ Precision: {precision:.3f}")
    print(f"ğŸ“¢ Recall: {recall:.3f}")
    print(f"ğŸ“Š F1 Score: {f1:.3f}")

    # è¿”å›åŸå§‹åˆå¹¶çš„æ•°æ®æ¡†ï¼ˆä¸å«é‡å¤ï¼‰ï¼Œè€Œä¸æ˜¯å¹³è¡¡åçš„æ•°æ®æ¡†
    merged_df = merged_df.drop_duplicates().drop(columns=["Has_Charger"])
    return merged_df, accuracy, precision, recall, f1


# **ğŸ“Œ ç»Ÿè®¡é¡µé¢å¸ƒå±€**
def create_stats_layout():
    return dbc.Container([
        dbc.Row([
            dbc.Col([
                html.Label("Rolling Window Size (Hours)"),
                dcc.Slider(
                    id="window-size-slider",
                    min=1, max=12, step=1, value=4,
                    marks={i: str(i) for i in range(1, 13)}
                )
            ], width=6),

            dbc.Col([
                html.Label("Threshold Value"),
                dcc.Input(
                    id="threshold-input",
                    type="number",
                    value=1.5,
                    step=0.1
                )
            ], width=6),
        ], className="mb-4"),

        dbc.Row([
            dbc.Col([
                dbc.Button("Calculate", id="calculate-btn", color="primary", className="mb-3")
            ], width=12, className="text-center")
        ]),

        dbc.Row([
            dbc.Col([
                html.H4("Model Performance Metrics"),
                html.H5("Accuracy: ", id="accuracy-output", className="text-primary"),
                html.H5("Precision: ", id="precision-output", className="text-primary"),
                html.H5("Recall: ", id="recall-output", className="text-primary"),
                html.H5("F1 Score: ", id="f1-score-output", className="text-primary")
            ], width=12, className="text-center mt-4")
        ]),

        dbc.Row([
            dbc.Col(dash_table.DataTable(
                id='location-prob-table',
                columns=[
                    {"name": "LOCATION", "id": "LOCATION"},
                    {"name": "Probability", "id": "Probability"},
                    {"name": "Weeks", "id": "Weeks"},
                    {"name": "# of Chargers", "id": "# of Chargers"},
                    {"name": "Prediction", "id": "Prediction"},
                    # {"name": "Has_Charger", "id": "Has_Charger"}
                ],
                style_table={'overflowX': 'auto'},
                style_cell={'textAlign': 'left'},
                page_size=10
            ), width=12)
        ])
    ], fluid=True)