import os
import glob
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import time


def process_weekly_csv(window_size, initial_threshold=1.0):
    """
    Enhanced algorithm for detecting EV charging stations from electricity usage data.
    Implements seasonal baselines and ML-based thresholds.

    Args:
        window_size: Number of consecutive hours required to exceed threshold
        initial_threshold: Starting threshold value (will be optimized by ML)

    Returns:
        DataFrame with prediction results and performance metrics
    """
    start_time = time.time()
    input_dir = "/Users/jackson/BHI/Decrypted_Files/weekly_csv_files_test"
    file_paths = sorted(glob.glob(os.path.join(input_dir, "week_2023-*.csv")))
    print(f"ğŸ§ Found {len(file_paths)} files")

    if len(file_paths) == 0:
        print("âŒ No CSV files found in the directory!")
        return pd.DataFrame(columns=["LOCATION", "Probability", "Weeks"]), 0, 0, 0, 0

    # é¦–å…ˆæ”¶é›†æ‰€æœ‰ä½ç½®çš„ç‰¹å¾å’Œæ•°æ®
    all_features = []
    all_locations = []
    location_raw_data = {}  # å­˜å‚¨åŸå§‹æ•°æ®ï¼Œç¨åç”¨äºåˆ¤æ–­
    total_weeks = len(file_paths)

    # åˆ›å»ºå­£èŠ‚æ˜ å°„: 1-3æœˆä¸ºå†¬å­£, 4-6æœˆä¸ºæ˜¥å­£, 7-9æœˆä¸ºå¤å­£, 10-12æœˆä¸ºç§‹å­£
    def get_season(date_str):
        month = int(date_str.split('-')[1])
        if 1 <= month <= 3:
            return 'winter'
        elif 4 <= month <= 6:
            return 'spring'
        elif 7 <= month <= 9:
            return 'summer'
        else:
            return 'fall'

    # ç¬¬ä¸€æ­¥ï¼šæ”¶é›†ç‰¹å¾å’ŒæŒ‰å­£èŠ‚åˆ†ç»„æ•°æ®
    for file in file_paths:
        try:
            date_str = os.path.basename(file).split("_")[1].split(".")[0]
            season = get_season(date_str)
            df = pd.read_csv(file, low_memory=False)
            df.columns = df.columns.str.strip()

            if "LOCATION" not in df.columns:
                print(f"âš ï¸ Skipping {file}: 'LOCATION' column not found!")
                continue

            # å¯¹æ¯ä¸ª location è¿›è¡Œå¤„ç†
            for location, group in df.groupby("LOCATION"):
                usage_matrix = group.iloc[:, 4:28].astype(float).values  # æå–è¯¥åœ°ç‚¹çš„24å°æ—¶ç”¨ç”µæ•°æ®

                # å¤„ç†NaNå€¼
                if np.isnan(usage_matrix).any():
                    usage_matrix = np.nan_to_num(usage_matrix, nan=0.0)

                # åˆ›å»ºä½ç½®æ•°æ®ç»“æ„
                if location not in all_locations:
                    all_locations.append(location)
                    # åˆå§‹åŒ–å­˜å‚¨ç»“æ„
                    location_raw_data[location] = {
                        "weeks": {},
                        "seasons": {"winter": {"hours": [[] for _ in range(24)]},
                                    "spring": {"hours": [[] for _ in range(24)]},
                                    "summer": {"hours": [[] for _ in range(24)]},
                                    "fall": {"hours": [[] for _ in range(24)]}},
                        "features": {}
                    }

                    # æå–ç‰¹å¾
                    mean_usage = np.mean(usage_matrix)
                    std_dev = np.std(usage_matrix)
                    max_usage = np.max(usage_matrix)

                    # æ—¶æ®µç‰¹å¾
                    morning_avg = np.mean(usage_matrix[:, 6:12])  # 6AM-12PM
                    afternoon_avg = np.mean(usage_matrix[:, 12:18])  # 12PM-6PM
                    evening_avg = np.mean(usage_matrix[:, 18:24])  # 6PM-12AM
                    night_avg = np.mean(usage_matrix[:, 0:6])  # 12AM-6AM

                    # æ¯”ç‡ç‰¹å¾
                    peak_to_avg = max_usage / mean_usage if mean_usage > 0 else 0
                    evening_to_day = evening_avg / (morning_avg + afternoon_avg) if (
                                                                                                morning_avg + afternoon_avg) > 0 else 0

                    # å­˜å‚¨ç‰¹å¾
                    location_raw_data[location]["features"] = {
                        "overall_mean": mean_usage,
                        "std_dev": std_dev,
                        "max_usage": max_usage,
                        "morning_avg": morning_avg,
                        "afternoon_avg": afternoon_avg,
                        "evening_avg": evening_avg,
                        "night_avg": night_avg
                    }

                    # åŠ å…¥ç‰¹å¾åˆ—è¡¨
                    all_features.append([
                        mean_usage, std_dev, max_usage,
                        morning_avg, afternoon_avg, evening_avg, night_avg,
                        peak_to_avg, evening_to_day
                    ])

                # å­˜å‚¨æ¯å‘¨çš„åŸå§‹æ•°æ®
                location_raw_data[location]["weeks"][date_str] = {
                    "usage_matrix": usage_matrix,
                    "season": season
                }

                # æŒ‰å­£èŠ‚å’Œå°æ—¶æ”¶é›†æ•°æ®
                for day_idx in range(usage_matrix.shape[0]):
                    for hour_idx in range(24):
                        hour_value = usage_matrix[day_idx, hour_idx]
                        location_raw_data[location]["seasons"][season]["hours"][hour_idx].append(hour_value)

        except Exception as e:
            print(f"âŒ Error processing file {file}: {e}")

    # è®¡ç®—æ¯ä¸ªä½ç½®çš„å­£èŠ‚æ€§åŸºçº¿
    for location in all_locations:
        # åˆå§‹åŒ–å­£èŠ‚åŸºçº¿
        season_baselines = {}

        # å¯¹æ¯ä¸ªå­£èŠ‚è®¡ç®—24å°æ—¶çš„å¹³å‡ç”¨ç”µé‡
        for season in location_raw_data[location]["seasons"]:
            hourly_baseline = np.zeros(24)

            for hour in range(24):
                hour_data = location_raw_data[location]["seasons"][season]["hours"][hour]
                if len(hour_data) > 0:
                    hourly_baseline[hour] = np.mean(hour_data)
                else:
                    # å¦‚æœè¯¥å­£èŠ‚è¯¥å°æ—¶æ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨æ€»ä½“å¹³å‡å€¼
                    hourly_baseline[hour] = location_raw_data[location]["features"]["overall_mean"]

            season_baselines[season] = hourly_baseline

        # å­˜å‚¨å­£èŠ‚æ€§åŸºçº¿
        location_raw_data[location]["baselines"] = season_baselines

    # åˆ›å»ºç‰¹å¾æ•°æ®æ¡†
    features_df = pd.DataFrame(all_features, columns=[
        "Mean_Usage", "Std_Dev", "Max_Usage",
        "Morning_Avg", "Afternoon_Avg", "Evening_Avg", "Night_Avg",
        "Peak_To_Avg_Ratio", "Evening_To_Day_Ratio"
    ])

    # ç¬¬äºŒæ­¥ï¼šå‡†å¤‡è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹çš„æ•°æ®
    try:
        # è·å–çœŸå®æ ‡ç­¾
        real_data = pd.read_csv(file_paths[-1], usecols=["LOCATION", "# of Chargers"])
        real_data["Has_Charger"] = real_data["# of Chargers"].notna().astype(int)

        # åˆå¹¶ç‰¹å¾å’Œæ ‡ç­¾
        ml_data = pd.DataFrame({"LOCATION": all_locations})
        ml_data = ml_data.merge(real_data[["LOCATION", "Has_Charger"]], on="LOCATION", how="left").fillna(0)

        # æ·»åŠ ç‰¹å¾
        for i, col in enumerate(features_df.columns):
            ml_data[col] = features_df[col].values


        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        X = ml_data.drop(["LOCATION", "Has_Charger"], axis=1)
        y = ml_data["Has_Charger"]

        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)

        # æ‹†åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.2, random_state=42, stratify=y
        )

        # ä¼˜åŒ–éšæœºæ£®æ—åˆ†ç±»å™¨å‚æ•°ä»¥åŠ å¿«è®¡ç®—é€Ÿåº¦
        model_start_time = time.time()
        classifier = RandomForestClassifier(
            n_estimators=50,  # å‡å°‘æ ‘çš„æ•°é‡ï¼ˆä»100å‡å°‘åˆ°50ï¼‰
            max_depth=8,      # é™åˆ¶æ ‘çš„æœ€å¤§æ·±åº¦
            min_samples_split=5,  # å¢åŠ åˆ†è£‚æ‰€éœ€çš„æœ€å°æ ·æœ¬æ•°
            min_samples_leaf=2,   # è®¾ç½®å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°
            max_features='sqrt',  # ä½¿ç”¨ç‰¹å¾çš„å¹³æ–¹æ ¹æ•°é‡
            bootstrap=True,       # ä½¿ç”¨è‡ªåŠ©é‡‡æ ·
            n_jobs=-1,            # ä½¿ç”¨æ‰€æœ‰CPUæ ¸å¿ƒå¹¶è¡Œè®¡ç®—
            random_state=42
        )
        classifier.fit(X_train, y_train)
        model_train_time = time.time() - model_start_time
        print(f"åˆ†ç±»å™¨è®­ç»ƒæ—¶é—´: {model_train_time:.2f} ç§’")

        # è®¡ç®—æµ‹è¯•é›†æ€§èƒ½
        y_pred = classifier.predict(X_test)
        print(f"\næ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡: {accuracy_score(y_test, y_pred) * 100:.2f}%")

        # æå–ç‰¹å¾é‡è¦æ€§
        feature_importance = pd.DataFrame({
            'Feature': X.columns,
            'Importance': classifier.feature_importances_
        }).sort_values('Importance', ascending=False)

        print("\nç‰¹å¾é‡è¦æ€§:")
        for i, row in feature_importance.head(5).iterrows():  # åªæ˜¾ç¤ºå‰5ä¸ªé‡è¦ç‰¹å¾
            print(f"{row['Feature']:20}: {row['Importance']:.4f}")

        # ä¼˜åŒ–é˜ˆå€¼é¢„æµ‹æ¨¡å‹
        threshold_model = RandomForestRegressor(
            n_estimators=30,      # æ›´å°‘çš„æ ‘
            max_depth=6,          # è¾ƒæµ…çš„æ ‘
            min_samples_split=5,
            max_features='sqrt',
            n_jobs=-1,
            random_state=42
        )

        # åˆ›å»ºé˜ˆå€¼ç›®æ ‡ï¼šæœ‰å……ç”µæ¡©çš„ä½ç½®è®¾ç½®è¾ƒé«˜é˜ˆå€¼ï¼Œæ²¡æœ‰çš„è®¾ç½®è¾ƒä½é˜ˆå€¼
        threshold_target = np.array([initial_threshold * (1.5 if val == 1 else 0.8) for val in y])

        # è®­ç»ƒæ¨¡å‹
        threshold_model_start_time = time.time()
        threshold_model.fit(X_scaled, threshold_target)
        threshold_model_train_time = time.time() - threshold_model_start_time
        print(f"é˜ˆå€¼æ¨¡å‹è®­ç»ƒæ—¶é—´: {threshold_model_train_time:.2f} ç§’")

        # é¢„æµ‹æ¯ä¸ªä½ç½®çš„æœ€ä½³é˜ˆå€¼
        predicted_thresholds = threshold_model.predict(X_scaled)
        location_thresholds = dict(zip(all_locations, predicted_thresholds))

        print("\né¢„æµ‹çš„é˜ˆå€¼èŒƒå›´:", np.min(predicted_thresholds), "åˆ°", np.max(predicted_thresholds))

    except Exception as e:
        print(f"âŒ Error training ML model: {e}")
        # å¦‚æœæœºå™¨å­¦ä¹ æ¨¡å‹å¤±è´¥ï¼Œå›é€€åˆ°é»˜è®¤é˜ˆå€¼
        location_thresholds = {loc: initial_threshold for loc in all_locations}

    # ç¬¬ä¸‰æ­¥ï¼šä½¿ç”¨å­£èŠ‚æ€§åŸºçº¿å’ŒMLé¢„æµ‹çš„é˜ˆå€¼è¿›è¡Œåˆ¤æ–­
    location_data = {}
    for location in all_locations:
        # è·å–è¯¥ä½ç½®çš„MLé¢„æµ‹é˜ˆå€¼
        ml_threshold = location_thresholds.get(location, initial_threshold)

        # åˆå§‹åŒ–ä½ç½®æ•°æ®
        location_data[location] = {
            "Exceed_Count": 0,
            "Weeks": [],
            "Features": location_raw_data[location]["features"],
            "ML_Threshold": ml_threshold
        }

        # å¯¹æ¯å‘¨æ•°æ®åº”ç”¨å­£èŠ‚æ€§åŸºçº¿å’ŒMLé˜ˆå€¼
        for week_date, week_data in location_raw_data[location]["weeks"].items():
            usage_matrix = week_data["usage_matrix"]
            season = week_data["season"]

            # è·å–è¯¥å­£èŠ‚çš„24å°æ—¶åŸºçº¿
            seasonal_baseline = location_raw_data[location]["baselines"][season]

            # åˆ¤æ–­æ˜¯å¦æœ‰è¿ç»­window_sizeä¸ªå°æ—¶è¶…è¿‡å­£èŠ‚åŸºçº¿+MLé˜ˆå€¼
            condition_met = False

            for day_idx, row in enumerate(usage_matrix):
                for start_hour in range(len(row) - window_size + 1):
                    # æ£€æŸ¥è¿ç»­window_sizeä¸ªå°æ—¶æ˜¯å¦éƒ½è¶…è¿‡äº†å¯¹åº”å°æ—¶çš„å­£èŠ‚åŸºçº¿+MLé˜ˆå€¼
                    hours_exceeded = True
                    for i in range(window_size):
                        hour_idx = start_hour + i
                        if row[hour_idx] <= (seasonal_baseline[hour_idx] + ml_threshold):
                            hours_exceeded = False
                            break

                    if hours_exceeded:
                        condition_met = True
                        break

                if condition_met:
                    break

            if condition_met:
                location_data[location]["Exceed_Count"] += 1
                location_data[location]["Weeks"].append(week_date)

    # è®¡ç®—æ¦‚ç‡
    prob_data = []
    for loc, data in location_data.items():
        # è®¡ç®—åŸºçº¿çš„å¹³å‡å€¼ï¼ˆç”¨äºå±•ç¤ºï¼‰
        avg_baselines = {}
        for season in location_raw_data[loc]["baselines"]:
            avg_baselines[f"{season}_baseline"] = np.mean(location_raw_data[loc]["baselines"][season])

        prob_data.append({
            "LOCATION": loc,
            "Probability": round(data["Exceed_Count"] / total_weeks, 3) if total_weeks > 0 else 0,
            "Weeks": ", ".join(data["Weeks"]),
            "Mean_Usage": data["Features"]["overall_mean"],
            "ML_Threshold": data["ML_Threshold"],
            **avg_baselines  # æ·»åŠ æ‰€æœ‰å­£èŠ‚çš„å¹³å‡åŸºçº¿
        })

    prob_df = pd.DataFrame(prob_data)

    # è¯»å–çœŸå®å……ç”µæ¡©æ•°æ®è¿›è¡Œè¯„ä¼°
    try:
        real_data = pd.read_csv(file_paths[-1], usecols=["LOCATION", "# of Chargers"])
        real_data["Has_Charger"] = real_data["# of Chargers"].notna().astype(int)
    except Exception as e:
        print(f"âŒ Error reading charger data: {e}")
        real_data = pd.DataFrame(columns=["LOCATION", "# of Chargers", "Has_Charger"])

    # åˆå¹¶é¢„æµ‹æ•°æ®å’ŒçœŸå®æ•°æ®
    merged_df = prob_df.merge(real_data, on="LOCATION", how="left").fillna(0)
    merged_df["Prediction"] = (merged_df["Probability"] > 0.5).astype(int)

    # è¯„ä¼°éƒ¨åˆ†
    eval_df = merged_df.copy()
    tp_fn = eval_df[eval_df["Has_Charger"] >= 1]
    fp_tn = eval_df[eval_df["Has_Charger"] == 0]

    tp_fn_count = len(tp_fn)
    fp_tn_count = len(fp_tn)

    if tp_fn_count == 0 or fp_tn_count == 0:
        print("âš ï¸ Warning: One of the classes has zero samples. Metrics may be invalid.")
        accuracy, precision, recall, f1 = 0, 0, 0, 0
    else:
        if tp_fn_count > fp_tn_count:
            tp_fn = tp_fn.sample(n=fp_tn_count, random_state=42)
        elif fp_tn_count > tp_fn_count:
            fp_tn = fp_tn.sample(n=tp_fn_count, replace=True, random_state=42)

        balanced_df = pd.concat([tp_fn, fp_tn])
        y_true = balanced_df["Has_Charger"]
        y_pred = balanced_df["Prediction"]

        accuracy = accuracy_score(y_true, y_pred) * 100

        try:
            precision = precision_score(y_true, y_pred) * 100
        except:
            precision = 0

        try:
            recall = recall_score(y_true, y_pred) * 100
        except:
            recall = 0

        try:
            f1 = f1_score(y_true, y_pred) * 100
        except:
            f1 = 0

        try:
            tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
            print(f"True Positive (TP): {tp}")
            print(f"False Positive (FP): {fp}")
            print(f"False Negative (FN): {fn}")
            print(f"True Negative (TN): {tn}")
        except:
            print("Could not compute confusion matrix")

    print(f"âœ… Accuracy: {accuracy:.3f}")
    print(f"ğŸ¯ Precision: {precision:.3f}")
    print(f"ğŸ“¢ Recall: {recall:.3f}")
    print(f"ğŸ“Š F1 Score: {f1:.3f}")

    total_time = time.time() - start_time
    print(f"â±ï¸ æ€»è¿è¡Œæ—¶é—´: {total_time:.2f} ç§’")

    merged_df = merged_df.drop_duplicates().drop(columns=["Has_Charger"])
    return merged_df, accuracy, precision, recall, f1


def evaluate_forest_classifier_direct(window_size, initial_threshold=1.0):
    """
    Directly use Random Forest Classifier to predict charger presence and evaluate.
    Returns: DataFrame with predictions and metrics (accuracy, precision, recall, F1).
    """
    input_dir = "/Users/jackson/BHI/Decrypted_Files/weekly_csv_files_test"
    file_paths = sorted(glob.glob(os.path.join(input_dir, "week_2023-*.csv")))
    if len(file_paths) == 0:
        print("âŒ No CSV files found in the directory!")
        return pd.DataFrame(), 0, 0, 0, 0

    # Collect features as in process_weekly_csv
    all_features = []
    all_locations = []
    for file in file_paths:
        try:
            df = pd.read_csv(file, low_memory=False)
            df.columns = df.columns.str.strip()
            if "LOCATION" not in df.columns:
                continue
            for location, group in df.groupby("LOCATION"):
                if location not in all_locations:
                    usage_matrix = group.iloc[:, 4:28].astype(float).values
                    if np.isnan(usage_matrix).any():
                        usage_matrix = np.nan_to_num(usage_matrix, nan=0.0)
                    all_locations.append(location)
                    mean_usage = np.mean(usage_matrix)
                    std_dev = np.std(usage_matrix)
                    max_usage = np.max(usage_matrix)
                    morning_avg = np.mean(usage_matrix[:, 6:12])
                    afternoon_avg = np.mean(usage_matrix[:, 12:18])
                    evening_avg = np.mean(usage_matrix[:, 18:24])
                    night_avg = np.mean(usage_matrix[:, 0:6])
                    peak_to_avg = max_usage / mean_usage if mean_usage > 0 else 0
                    evening_to_day = evening_avg / (morning_avg + afternoon_avg) if (morning_avg + afternoon_avg) > 0 else 0
                    all_features.append([
                        mean_usage, std_dev, max_usage,
                        morning_avg, afternoon_avg, evening_avg, night_avg,
                        peak_to_avg, evening_to_day
                    ])
        except Exception as e:
            continue

    features_df = pd.DataFrame(all_features, columns=[
        "Mean_Usage", "Std_Dev", "Max_Usage",
        "Morning_Avg", "Afternoon_Avg", "Evening_Avg", "Night_Avg",
        "Peak_To_Avg_Ratio", "Evening_To_Day_Ratio"
    ])

    # Debug print to check dataset size
    print(f"Found {len(all_locations)} unique locations for analysis")

    # Get true labels
    try:
        real_data = pd.read_csv(file_paths[-1], usecols=["LOCATION", "# of Chargers"])
        real_data = real_data.drop_duplicates("LOCATION")
        real_data["Has_Charger"] = real_data["# of Chargers"].notna().astype(int)
    except Exception as e:
        print(f"âŒ Error reading charger data: {e}")
        return pd.DataFrame(), 0, 0, 0, 0

    ml_data = pd.DataFrame({"LOCATION": all_locations})
    ml_data = ml_data.merge(real_data[["LOCATION", "Has_Charger"]], on="LOCATION", how="left").fillna(0)
    for i, col in enumerate(features_df.columns):
        ml_data[col] = features_df[col].values

    # Print class distribution
    pos_count = ml_data["Has_Charger"].sum()
    neg_count = len(ml_data) - pos_count
    print(f"Original class distribution - Positive: {pos_count}, Negative: {neg_count}")

    # Balance the dataset before splitting
    pos_samples = ml_data[ml_data["Has_Charger"] >= 1]
    neg_samples = ml_data[ml_data["Has_Charger"] == 0]

    if len(pos_samples) == 0 or len(neg_samples) == 0:
        print("âš ï¸ Warning: One of the classes has zero samples. Cannot balance dataset.")
        balanced_data = ml_data
    else:
        # Balance by upsampling minority class to match majority class
        if len(pos_samples) < len(neg_samples):
            # Upsample positive class
            pos_upsampled = pos_samples.sample(n=len(neg_samples), replace=True, random_state=42)
            balanced_data = pd.concat([pos_upsampled, neg_samples])
        elif len(neg_samples) < len(pos_samples):
            # Upsample negative class
            neg_upsampled = neg_samples.sample(n=len(pos_samples), replace=True, random_state=42)
            balanced_data = pd.concat([pos_samples, neg_upsampled])
        else:
            # Already balanced
            balanced_data = ml_data

    print(f"Balanced class distribution - Total: {len(balanced_data)}, Positive: {balanced_data['Has_Charger'].sum()}, Negative: {len(balanced_data) - balanced_data['Has_Charger'].sum()}")

    X = balanced_data.drop(["LOCATION", "Has_Charger"], axis=1)
    y = balanced_data["Has_Charger"]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )

    # Check train/test split distribution
    print(f"Training set - Positive: {y_train.sum()}, Negative: {len(y_train) - y_train.sum()}")
    print(f"Test set - Positive: {y_test.sum()}, Negative: {len(y_test) - y_test.sum()}")

    classifier = RandomForestClassifier(
        n_estimators=50,
        max_depth=8,
        min_samples_split=5,
        min_samples_leaf=2,
        max_features='sqrt',
        bootstrap=True,
        n_jobs=-1,
        random_state=42
    )
    classifier.fit(X_train, y_train)
    y_pred = classifier.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred) * 100
    try:
        precision = precision_score(y_test, y_pred) * 100
    except:
        precision = 0
    try:
        recall = recall_score(y_test, y_pred) * 100
    except:
        recall = 0
    try:
        f1 = f1_score(y_test, y_pred) * 100
    except:
        f1 = 0

    try:
        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
        print(f"Forest Classifier - TP: {tp}, FP: {fp}, FN: {fn}, TN: {tn}")
    except:
        print("Could not compute confusion matrix for Forest Classifier")

    print(f"Forest Classifier Accuracy: {accuracy:.3f}")
    print(f"Forest Classifier Precision: {precision:.3f}")
    print(f"Forest Classifier Recall: {recall:.3f}")
    print(f"Forest Classifier F1 Score: {f1:.3f}")

    # Feature importance
    feature_importance = pd.DataFrame({
        'Feature': X.columns,
        'Importance': classifier.feature_importances_
    }).sort_values('Importance', ascending=False)

    print("\nForest Classifier Feature Importance:")
    for i, row in feature_importance.head(5).iterrows():
        print(f"{row['Feature']:20}: {row['Importance']:.4f}")

    # For output, add predictions to original data
    indices_to_loc = {i: loc for i, loc in enumerate(balanced_data["LOCATION"])}
    test_indices = y_test.index
    test_locs = [indices_to_loc[i] for i in test_indices]
    
    test_results = pd.DataFrame({
        "LOCATION": test_locs,
        "Has_Charger": y_test.values,
        "Prediction": y_pred
    })

    return test_results, accuracy, precision, recall, f1


if __name__ == "__main__":
    # Test the algorithm with different parameters
    # window_sizes = [4, 3, 2]
    # initial_thresholds = [1.0]
    #
    # results = []
    # for window_size in window_sizes:
    #     for threshold in initial_thresholds:
    #         print(f"\n===== Testing window_size={window_size}, initial_threshold={threshold} =====")
    #         _, accuracy, precision, recall, f1 = process_weekly_csv(window_size, threshold)
    #         results.append({
    #             "Window Size": window_size,
    #             "Initial Threshold": threshold,
    #             "Accuracy": f"{accuracy:.2f}%",
    #             "Precision": f"{precision:.2f}%",
    #             "Recall": f"{recall:.2f}%",
    #             "F1 Score": f"{f1:.2f}%"
    #         })
    #
    # # Print summary
    # results_df = pd.DataFrame(results)
    # print("\n===== Results Summary =====")
    # print(results_df)

    # Add direct forest classifier evaluation
    print("\n===== Direct Forest Classifier Evaluation =====")
    _, acc, prec, rec, f1 = evaluate_forest_classifier_direct(window_size=3)
    print(f"Forest Classifier - Accuracy: {acc:.2f}%, Precision: {prec:.2f}%, Recall: {rec:.2f}%, F1: {f1:.2f}%")





